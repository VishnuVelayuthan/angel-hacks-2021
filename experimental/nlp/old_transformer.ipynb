{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "id": "3nWaFN4hB9C-"
   },
   "outputs": [],
   "source": [
    "# @title Run this cell to install our libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "import os\n",
    "\n",
    "# set pandas viewing options\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "# pd.reset_option(\"max_colwidth\")\n",
    "\n",
    "# the source of our data is: https://github.com/nbertagnolli/counsel-chat\n",
    "# pretrained weights: 'https://drive.google.com/uc?export=download&id=1rR0HAOKgs0yGAyZwqeJkX1U3W8234BgR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "yNHAdT_iCMOc"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"Calculate the attention weights. \"\"\"\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # add the mask to zero out padding tokens\n",
    "    if mask is not None:\n",
    "        logits += mask * -1e9\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k)\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "srAlGpjHE72c"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = (\n",
    "            inputs[\"query\"],\n",
    "            inputs[\"key\"],\n",
    "            inputs[\"value\"],\n",
    "            inputs[\"mask\"],\n",
    "        )\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # linear layers\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # split heads\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # concatenation of heads\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        # final linear layer\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "kvhIjJ4dHoU_"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "form",
    "id": "KEwQvQR3Jteo"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model,\n",
    "        )\n",
    "        # apply sin to even index in the array\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # apply cos to odd index in the array\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, : tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "id": "o8jGQcv4OzBX"
   },
   "outputs": [],
   "source": [
    "# individual encoder layer\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    attention = MultiHeadAttention(d_model, num_heads, name=\"attention\")(\n",
    "        {\"query\": inputs, \"key\": inputs, \"value\": inputs, \"mask\": padding_mask}\n",
    "    )\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation=\"relu\")(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "\n",
    "# complete encoder architecture\n",
    "def encoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "form",
    "id": "bmaq4ZBiSWdc"
   },
   "outputs": [],
   "source": [
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    attention1 = MultiHeadAttention(d_model, num_heads, name=\"attention_1\")(\n",
    "        inputs={\n",
    "            \"query\": inputs,\n",
    "            \"key\": inputs,\n",
    "            \"value\": inputs,\n",
    "            \"mask\": look_ahead_mask,\n",
    "        }\n",
    "    )\n",
    "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    attention2 = MultiHeadAttention(d_model, num_heads, name=\"attention_2\")(\n",
    "        inputs={\n",
    "            \"query\": attention1,\n",
    "            \"key\": enc_outputs,\n",
    "            \"value\": enc_outputs,\n",
    "            \"mask\": padding_mask,\n",
    "        }\n",
    "    )\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(\n",
    "        attention2 + attention1\n",
    "    )\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation=\"relu\")(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "\n",
    "# decoder itself\n",
    "def decoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"decoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"decoder_layer_{}\".format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "form",
    "id": "CNoHySCeWl2e"
   },
   "outputs": [],
   "source": [
    "def transformer(\n",
    "    vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"transformer\"\n",
    "):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None), name=\"enc_padding_mask\"\n",
    "    )(inputs)\n",
    "    # mask the future tokens for decoder inputs at the 1st attention block\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask, output_shape=(1, None, None), name=\"look_ahead_mask\"\n",
    "    )(dec_inputs)\n",
    "    # mask the encoder outputs for the 2nd attention block\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None), name=\"dec_padding_mask\"\n",
    "    )(inputs)\n",
    "\n",
    "    enc_outputs = encoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "form",
    "id": "IjDbmBt0qYF0"
   },
   "outputs": [],
   "source": [
    "# load in our data with this code chunk:\n",
    "chat_data = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/nbertagnolli/counsel-chat/master/data/20200325_counsel_chat.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "s3CEuQ9Wgix_"
   },
   "outputs": [],
   "source": [
    "X = chat_data[\"questionText\"]\n",
    "y = chat_data[\"answerText\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "both",
    "id": "t2tzKvtdIDyF"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(phrase):\n",
    "    phrase = re.sub(r\"\\xa0\", \"\", phrase)  # removes \"\\xa0\"\n",
    "    phrase = re.sub(r\"\\n\", \"\", phrase)  # removes \"\\n\"\n",
    "    phrase = re.sub(\"[.]{1,}\", \".\", phrase)  # removes duplicate \".\"s\n",
    "    phrase = re.sub(\"[ ]{1,}\", \" \", phrase)  # removes duplicate spaces\n",
    "    return phrase\n",
    "\n",
    "\n",
    "# run cleaning function\n",
    "X = X.apply(preprocess_text)\n",
    "y = y.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "both",
    "id": "-I7FZuEAdVu_"
   },
   "outputs": [],
   "source": [
    "# run this code chunk, to store all of our question/answer pairs\n",
    "question_answer_pairs = []\n",
    "\n",
    "# loop through each combination of question + answer\n",
    "for (question, answer) in zip(X, y):\n",
    "\n",
    "    # clean up text inputs\n",
    "\n",
    "    # example:\n",
    "    # question = \"I am not feeling well today. I feel sad.\"\n",
    "    # answer = \"Tell me more about how you feel. What have you been up to today?\"\n",
    "\n",
    "    question = preprocess_text(question)\n",
    "    answer = preprocess_text(answer)\n",
    "\n",
    "    # split by .\n",
    "    # example\n",
    "    # question_arr = [\"I am not feeling well today\", \"I feel sad\"]\n",
    "    # answer_arr = [\"Tell me more about how you feel\", \"What have you been up to?\"]\n",
    "    question_arr = question.split(\".\")\n",
    "    answer_arr = answer.split(\".\")\n",
    "\n",
    "    # get the maximum length, which will be the shorter of the two\n",
    "    max_sentences = min(len(question_arr), len(answer_arr))\n",
    "\n",
    "    # for each combination of question + answer, pair them up\n",
    "    for i in range(max_sentences):\n",
    "\n",
    "        # set up Q/A pair\n",
    "        q_a_pair = []\n",
    "\n",
    "        # append question, answer to pair (e.g,. first sentence of question + first sentence of answer, etc.)\n",
    "        q_a_pair.append(question_arr[i])\n",
    "        q_a_pair.append(answer_arr[i])\n",
    "\n",
    "        # append to question_answer_pairs\n",
    "        question_answer_pairs.append(q_a_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellView": "form",
    "id": "w4CbtUGS7dNf"
   },
   "outputs": [],
   "source": [
    "# Build tokenizer using tfds for both questions and answers\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    [arr[0] + arr[1] for arr in question_answer_pairs], target_vocab_size=2 ** 13\n",
    ")\n",
    "\n",
    "# Define start and end token to indicate the start and end of a sentence\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "# Vocabulary size plus start and end token\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellView": "form",
    "id": "skfHWsGWuPkT"
   },
   "outputs": [],
   "source": [
    "# maximum sentence length\n",
    "MAX_LENGTH = 100  # chosen arbitrarily\n",
    "\n",
    "# tokenize, filter, pad sentences\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    \"\"\"\n",
    "    Tokenize, filter, and pad our inputs and outputs\n",
    "  \"\"\"\n",
    "\n",
    "    # store results\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "    # loop through inputs, outputs\n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "\n",
    "        # tokenize sentence\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "        # check tokenized sentence max length\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "\n",
    "    # pad tokenized sentences\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding=\"post\"\n",
    "    )\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_outputs, maxlen=MAX_LENGTH, padding=\"post\"\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellView": "form",
    "id": "4dnCQ0dfxnbO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8589\n",
      "Number of samples: 8723\n",
      "<PrefetchDataset shapes: ({inputs: (None, 100), dec_inputs: (None, 99)}, {outputs: (None, 99)}), types: ({inputs: tf.int32, dec_inputs: tf.int32}, {outputs: tf.int32})>\n"
     ]
    }
   ],
   "source": [
    "# get questions, answers\n",
    "questions, answers = tokenize_and_filter(\n",
    "    [arr[0] for arr in question_answer_pairs], [arr[1] for arr in question_answer_pairs]\n",
    ")\n",
    "\n",
    "print(\"Vocab size: {}\".format(VOCAB_SIZE))\n",
    "print(\"Number of samples: {}\".format(len(questions)))\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# decoder inputs use the previous target as input\n",
    "# remove START_TOKEN from targets\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ({\"inputs\": questions, \"dec_inputs\": answers[:, :-1]}, {\"outputs\": answers[:, 1:]},)\n",
    ")\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-eY67bx0j4Mf"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "D_MODEL = 256\n",
    "model = transformer(VOCAB_SIZE, 2, 512, D_MODEL, 8, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellView": "form",
    "id": "xVRbcgolbDqY"
   },
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=\"none\"\n",
    "    )(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "cellView": "form",
    "id": "U6gBvdLBkNTV"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9\n",
    ")\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "H_UnprRgb5fy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "137/137 [==============================] - 17s 128ms/step - loss: 2.0903 - accuracy: 0.0060\n",
      "Epoch 2/20\n",
      "137/137 [==============================] - 18s 130ms/step - loss: 1.8170 - accuracy: 0.0139\n",
      "Epoch 3/20\n",
      "137/137 [==============================] - 18s 129ms/step - loss: 1.5625 - accuracy: 0.0164\n",
      "Epoch 4/20\n",
      "137/137 [==============================] - 18s 131ms/step - loss: 1.4513 - accuracy: 0.0296\n",
      "Epoch 5/20\n",
      "137/137 [==============================] - 20s 145ms/step - loss: 1.3199 - accuracy: 0.0420\n",
      "Epoch 6/20\n",
      "137/137 [==============================] - 17s 126ms/step - loss: 1.2305 - accuracy: 0.0472\n",
      "Epoch 7/20\n",
      "137/137 [==============================] - 17s 126ms/step - loss: 1.1723 - accuracy: 0.0517\n",
      "Epoch 8/20\n",
      "137/137 [==============================] - 17s 126ms/step - loss: 1.1247 - accuracy: 0.0558\n",
      "Epoch 9/20\n",
      "137/137 [==============================] - 17s 126ms/step - loss: 1.0828 - accuracy: 0.0589\n",
      "Epoch 10/20\n",
      "137/137 [==============================] - 18s 135ms/step - loss: 1.0429 - accuracy: 0.0620\n",
      "Epoch 11/20\n",
      "137/137 [==============================] - 19s 142ms/step - loss: 1.0029 - accuracy: 0.0655\n",
      "Epoch 12/20\n",
      "137/137 [==============================] - 20s 144ms/step - loss: 0.9629 - accuracy: 0.0690\n",
      "Epoch 13/20\n",
      "137/137 [==============================] - 20s 146ms/step - loss: 0.9210 - accuracy: 0.0730\n",
      "Epoch 14/20\n",
      "137/137 [==============================] - 19s 136ms/step - loss: 0.8778 - accuracy: 0.0774\n",
      "Epoch 15/20\n",
      "137/137 [==============================] - 18s 132ms/step - loss: 0.8332 - accuracy: 0.0822\n",
      "Epoch 16/20\n",
      "137/137 [==============================] - 18s 132ms/step - loss: 0.7850 - accuracy: 0.0878\n",
      "Epoch 17/20\n",
      "137/137 [==============================] - 18s 129ms/step - loss: 0.7358 - accuracy: 0.0939\n",
      "Epoch 18/20\n",
      "137/137 [==============================] - 18s 130ms/step - loss: 0.6870 - accuracy: 0.0999\n",
      "Epoch 19/20\n",
      "137/137 [==============================] - 18s 129ms/step - loss: 0.6369 - accuracy: 0.1069\n",
      "Epoch 20/20\n",
      "137/137 [==============================] - 18s 128ms/step - loss: 0.5870 - accuracy: 0.1143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2039b2fda60>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdDx0756cQre"
   },
   "source": [
    "So... do we just end here? Well, we just so happen to have a version of this model that was trained on this same data. So we'll evaluate that instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./model_saves/old_chatbot_transformer_v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "cellView": "form",
    "id": "womlk5KiciwH"
   },
   "outputs": [],
   "source": [
    "# @title Run this cell to import our pretrained model\n",
    "\n",
    "model.load_weights(\"./model_saves/old_chatbot_transformer_v1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHjd1CTNcgls"
   },
   "source": [
    "### Evaluating our model\n",
    "\n",
    "Now that we have a working model, let's start playing around with it and see how it does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "form",
    "id": "RK_yXbWFcix7"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    sentence = preprocess_text(sentence)\n",
    "\n",
    "    sentence = tf.expand_dims(\n",
    "        START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0\n",
    "    )\n",
    "\n",
    "    output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        predictions = model(inputs=[sentence, output], training=False)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        # concatenated the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0)\n",
    "\n",
    "\n",
    "def predict(sentence):\n",
    "    prediction = evaluate(sentence)\n",
    "\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size]\n",
    "    )\n",
    "\n",
    "    print(\"Input: {}\".format(sentence))\n",
    "    print(\"Output: {}\".format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "cellView": "form",
    "id": "t2NgvQVkd3v9"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input Sentence:  I am sad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Input: I am sad\n",
      "Output: yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling yelling firfirfirfir\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "sentence = input(\"Input Sentence: \")\n",
    "print(\"--------------------\")\n",
    "# output = predict(sentence)\n",
    "_ = predict(sentence)\n",
    "print(\"--------------------\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Student module_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:inspirit-ai]",
   "language": "python",
   "name": "conda-env-inspirit-ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
